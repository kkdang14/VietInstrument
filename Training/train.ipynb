{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d124af23",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "\n",
    "In this notebook, we will utilize several libraries to build and train a multimodal question-answering model. Below is a brief description of the key libraries used:\n",
    "\n",
    "- **os**: For interacting with the operating system, such as file path management.\n",
    "- **torch**: The PyTorch library for building and training deep learning models.\n",
    "- **torch.nn**: Provides modules and classes for building neural networks.\n",
    "- **pandas**: For data manipulation and analysis.\n",
    "- **torch.utils.data**: For creating custom datasets and data loaders.\n",
    "- **transformers**: Hugging Face library for pre-trained transformer models like ViT and PhoBERT.\n",
    "- **PIL (Pillow)**: For image processing.\n",
    "- **sklearn.preprocessing**: For preprocessing tasks like label encoding.\n",
    "\n",
    "These libraries will help us preprocess data, define the model architecture, and train the model efficiently.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c4c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel, ViTImageProcessor, AutoTokenizer, AutoModel\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4adc75a",
   "metadata": {},
   "source": [
    "## Setting Up Device\n",
    "\n",
    "In this section, we configure the device to utilize GPU if available. This ensures faster computations, especially when working with deep learning models. If a GPU is not available, the code will fall back to using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2df4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36827c3",
   "metadata": {},
   "source": [
    "## Loading Pre-trained Models\n",
    "\n",
    "In this section, we load the pre-trained ViT (Vision Transformer) and PhoBERT models. These models will serve as the backbone for extracting visual and textual features, respectively. The ViT model is used for image feature extraction, while PhoBERT is utilized for processing textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ViT model and feature extractor\n",
    "vit_path = \"google/vit-base-patch16-224-in21k\"\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(vit_path)\n",
    "model_vit = ViTModel.from_pretrained(vit_path).to(device)\n",
    "\n",
    "# Load PhoBERT model and tokenizer\n",
    "phobert_path = \"vinai/phobert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(phobert_path)\n",
    "model_bert = AutoModel.from_pretrained(phobert_path).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a81262",
   "metadata": {},
   "source": [
    "## Defining the Custom Model\n",
    "\n",
    "In this section, we define a custom model that combines the Vision Transformer (ViT) and PhoBERT for multimodal question answering. The model integrates visual and textual features using a fusion mechanism and outputs predictions based on the combined features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb1112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.le = LabelEncoder()\n",
    "        self.data['encoded_answer'] = self.le.fit_transform(self.data['answer'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Process image\n",
    "        image = Image.open(row['image_path']).convert(\"RGB\")\n",
    "        image_inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        # Process question text\n",
    "        text_inputs = tokenizer(row['question'], \n",
    "                                return_tensors=\"pt\", \n",
    "                                truncation=True, \n",
    "                                padding='max_length', \n",
    "                                max_length=128)\n",
    "        \n",
    "        label = torch.tensor(row['encoded_answer'], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'image_pixel_values': image_inputs['pixel_values'].squeeze(0),\n",
    "            'input_ids': text_inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': text_inputs['attention_mask'].squeeze(0),\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7c030",
   "metadata": {},
   "source": [
    "## Custom Collate Function\n",
    "\n",
    "In this section, we define a custom collate function to handle batching for the multimodal dataset. This function ensures that the image pixel values, input IDs, attention masks, and labels are properly stacked into tensors for efficient processing during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bf1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    image_pixel_values = torch.stack([item['image_pixel_values'] for item in batch])\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'image_pixel_values': image_pixel_values,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = QADataset(\"/kaggle/input/data-instruments/instrument_train/instrument_train.csv\")\n",
    "val_dataset = QADataset(\"/kaggle/input/data-instruments/instrument_val/instrument_val.csv\")\n",
    "test_dataset = QADataset(\"/kaggle/input/data-instruments/instrument_test/instrument_test.csv\")\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Save the label encoder classes for inference\n",
    "label_classes = train_dataset.le.classes_\n",
    "print(f\"Number of classes: {len(label_classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f1bee8",
   "metadata": {},
   "source": [
    "## Cross-Attention Module\n",
    "\n",
    "This section defines the Cross-Attention module, which is a key component of the multimodal fusion model. The Cross-Attention mechanism allows the model to attend to relevant features across different modalities (e.g., vision and text) by computing attention scores between query, key, and value tensors. This enables effective integration of visual and textual information for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173efc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, dropout=0.1):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.shape[0]\n",
    "        \n",
    "        # Project and reshape for multi-head attention\n",
    "        q = self.q_proj(q).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(k).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(v).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)  # Add head dimensions\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        out = self.out_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56edcc32",
   "metadata": {},
   "source": [
    "## Transformer Fusion Layer\n",
    "\n",
    "This section defines the Transformer Fusion Layer, which is responsible for integrating visual and textual features. It uses cross-attention mechanisms to allow the model to attend to relevant features across modalities, followed by feed-forward networks (FFN) for further processing. This layer is a crucial component of the multimodal fusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFusionLayer(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, dropout=0.1):\n",
    "        super(TransformerFusionLayer, self).__init__()\n",
    "        self.vis_to_text_attn = CrossAttention(dim, num_heads, dropout)\n",
    "        self.text_to_vis_attn = CrossAttention(dim, num_heads, dropout)\n",
    "        \n",
    "        self.vis_norm1 = nn.LayerNorm(dim)\n",
    "        self.vis_norm2 = nn.LayerNorm(dim)\n",
    "        self.text_norm1 = nn.LayerNorm(dim)\n",
    "        self.text_norm2 = nn.LayerNorm(dim)\n",
    "        \n",
    "        self.vis_ffn = nn.Sequential(\n",
    "            nn.Linear(dim, 4 * dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.text_ffn = nn.Sequential(\n",
    "            nn.Linear(dim, 4 * dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, vis_feats, text_feats, text_mask=None):\n",
    "        # Cross-attention from vision to text\n",
    "        vis_attn_output = self.vis_to_text_attn(vis_feats, text_feats, text_feats, text_mask)\n",
    "        vis_feats = self.vis_norm1(vis_feats + vis_attn_output)\n",
    "        \n",
    "        # FFN for vision features\n",
    "        vis_ffn_output = self.vis_ffn(vis_feats)\n",
    "        vis_output = self.vis_norm2(vis_feats + vis_ffn_output)\n",
    "        \n",
    "        # Cross-attention from text to vision\n",
    "        text_attn_output = self.text_to_vis_attn(text_feats, vis_feats, vis_feats)\n",
    "        text_feats = self.text_norm1(text_feats + text_attn_output)\n",
    "        \n",
    "        # FFN for text features\n",
    "        text_ffn_output = self.text_ffn(text_feats)\n",
    "        text_output = self.text_norm2(text_feats + text_ffn_output)\n",
    "        \n",
    "        return vis_output, text_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c2cbb",
   "metadata": {},
   "source": [
    "## Multimodal Fusion Model\n",
    "\n",
    "In this section, we define the multimodal fusion model that integrates visual and textual features using a combination of cross-attention mechanisms and feed-forward networks. The model is designed to handle both image and text inputs, enabling effective feature fusion for downstream tasks such as question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b2f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureProjector(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(FeatureProjector, self).__init__()\n",
    "        self.proj = nn.Linear(input_dim, output_dim)\n",
    "        self.norm = nn.LayerNorm(output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.norm(self.proj(x))\n",
    "    \n",
    "class FusionQAModel(nn.Module):\n",
    "    def __init__(self, output_dim, fusion_dim=768, num_fusion_layers=2):\n",
    "        super(FusionQAModel, self).__init__()\n",
    "        \n",
    "        # Project features to same dimension for fusion\n",
    "        self.vis_projector = FeatureProjector(768, fusion_dim)  # ViT hidden size -> fusion_dim\n",
    "        self.text_projector = FeatureProjector(768, fusion_dim) # PhoBERT hidden size -> fusion_dim\n",
    "        \n",
    "        # Fusion layers\n",
    "        self.fusion_layers = nn.ModuleList([\n",
    "            TransformerFusionLayer(fusion_dim) for _ in range(num_fusion_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim * 2, fusion_dim),\n",
    "            nn.LayerNorm(fusion_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(fusion_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, vis_feats, text_feats, text_attention_mask=None):\n",
    "        # Extract CLS token from ViT\n",
    "        vis_cls = vis_feats[:, 0]\n",
    "        \n",
    "        # Extract all token embeddings from PhoBERT\n",
    "        text_tokens = text_feats\n",
    "        text_cls = text_tokens[:, 0]\n",
    "        \n",
    "        # Project features to same dimension\n",
    "        vis_proj = self.vis_projector(vis_cls).unsqueeze(1)  # [B, 1, fusion_dim]\n",
    "        text_proj = self.text_projector(text_tokens)         # [B, seq_len, fusion_dim]\n",
    "        \n",
    "        # Apply fusion layers\n",
    "        for fusion_layer in self.fusion_layers:\n",
    "            vis_proj, text_proj = fusion_layer(vis_proj, text_proj, text_attention_mask)\n",
    "        \n",
    "        # Extract CLS tokens after fusion\n",
    "        vis_cls_fused = vis_proj.squeeze(1)\n",
    "        text_cls_fused = text_proj[:, 0]\n",
    "        \n",
    "        # Combine modalities for classification\n",
    "        combined_features = torch.cat([vis_cls_fused, text_cls_fused], dim=1)\n",
    "        logits = self.classifier(combined_features)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4579ad",
   "metadata": {},
   "source": [
    "## Complete Question Answering Model\n",
    "\n",
    "In this section, we define the `CompleteQAModel` class, which combines the pre-trained Vision Transformer (ViT), PhoBERT, and the FusionQAModel. This model integrates visual and textual features using the fusion mechanism and outputs predictions for multimodal question answering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteQAModel(nn.Module):\n",
    "    def __init__(self, vit_model, bert_model, fusion_model):\n",
    "        super(CompleteQAModel, self).__init__()\n",
    "        self.vit_model = vit_model\n",
    "        self.bert_model = bert_model\n",
    "        self.fusion_model = fusion_model\n",
    "        \n",
    "    def forward(self, image_pixel_values, input_ids, attention_mask):\n",
    "        # Get ViT features\n",
    "        vis_outputs = self.vit_model(pixel_values=image_pixel_values)\n",
    "        vis_feats = vis_outputs.last_hidden_state\n",
    "        \n",
    "        # Get PhoBERT features\n",
    "        text_outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_feats = text_outputs.last_hidden_state\n",
    "        \n",
    "        # Fusion and classification\n",
    "        logits = self.fusion_model(vis_feats, text_feats, attention_mask)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a226be",
   "metadata": {},
   "source": [
    "## Validate Model\n",
    "\n",
    "In this section, we define a function to validate the model's performance on the validation dataset. The function computes the average validation loss and accuracy, which are used to monitor the model's performance during training. It ensures that the model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def validate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            image_pixel_values = batch['image_pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(image_pixel_values, input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(dataloader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5e310",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "In this section, we define a function to evaluate the model's performance on the test dataset. The function computes the overall accuracy and returns the predictions and ground truth labels for further analysis. This step is crucial for assessing the model's generalization ability on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            image_pixel_values = batch['image_pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(image_pixel_values, input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2adc558",
   "metadata": {},
   "source": [
    "## Fusion Model, Complete Model, Optimizer, and Training\n",
    "\n",
    "In this section, we initialize the Fusion Model and Complete Model, configure the optimizer and loss function, and set up the training loop. We also handle loading checkpoints if they exist, allowing us to resume training from a saved state. The training process involves forward and backward passes, loss computation, and model parameter updates. At the end of each epoch, the model is validated on the validation dataset, and the learning rate is adjusted based on the validation loss. Checkpoints are saved periodically to ensure progress is not lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb54afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint path\n",
    "checkpoint_path = \"fusion_checkpoint.pth\"\n",
    "\n",
    "# Initialize the model\n",
    "output_dim = len(train_dataset.le.classes_)\n",
    "fusion_model = FusionQAModel(output_dim=output_dim, fusion_dim=768, num_fusion_layers=2).to(device)\n",
    "complete_model = CompleteQAModel(model_vit, model_bert, fusion_model).to(device)\n",
    "\n",
    "# Freeze base models (optional - can be adjusted based on your dataset size)\n",
    "for param in model_vit.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in model_bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optimizer and loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': fusion_model.parameters(), 'lr': 3e-4},\n",
    "    {'params': model_vit.parameters(), 'lr': 1e-5},\n",
    "    {'params': model_bert.parameters(), 'lr': 1e-5}\n",
    "], weight_decay=1e-3)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "# Load checkpoint if exists\n",
    "start_epoch = 0\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    complete_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming training from Epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"Starting new training...\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 15\n",
    "log_interval = 10  # Log every N batches\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    complete_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        # Move data to device\n",
    "        image_pixel_values = batch['image_pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = complete_model(image_pixel_values, input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record loss\n",
    "        epoch_loss += loss.item()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Logging\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # End of epoch validation\n",
    "    val_loss, val_acc = validate_model(complete_model, val_dataloader, criterion)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print epoch stats\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] complete - Train Loss: {epoch_loss/len(train_dataloader):.4f}, '\n",
    "            f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%')\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': complete_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = \"final_fusion_qa_model.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': complete_model.state_dict(),\n",
    "    'label_encoder_classes': label_classes,\n",
    "    'config': {\n",
    "        'fusion_dim': 768,\n",
    "        'num_fusion_layers': 2,\n",
    "        'output_dim': output_dim\n",
    "    }\n",
    "}, final_model_path)\n",
    "print(f\"Final model saved at {final_model_path}\")\n",
    "print(\"Evaluating on test set...\")\n",
    "all_preds, all_labels = evaluate_model(complete_model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
